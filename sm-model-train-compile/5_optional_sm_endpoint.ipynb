{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4. Compile the pretrained PyTorch model with SageMaker Neo\n",
    "---\n",
    "\n",
    "In this notebook, we compile and deploy the model trained in the previous session with SageMaker Neo. SageMaker Neo is an API that optimizes machine learning models for hardware, and models compiled with Neo can run anywhere in the cloud and on edge devices.\n",
    "\n",
    "As of August 2021, SageMaker Neo supports PyTorch 1.6.0 on cloud instances and edge devices, and PyTorch 1.5.1 on AWS Inferentia.\n",
    "\n",
    "Please refer to the link below for instance types, hardware, and deep learning frameworks supported by SageMaker Neo.\n",
    "\n",
    "- Cloud Instance: https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html\n",
    "- Edge Device: https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html\n",
    "\n",
    "\n",
    "This hands-on can be completed in about **15 minutes to 30 minutes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    base_model_name\n",
    "    local_model_path\n",
    "    model_name\n",
    "    num_classes\n",
    "    print(\"[OK] You can proceed.\")\n",
    "except NameError:\n",
    "    print(\"+\"*60)\n",
    "    print(\"[ERROR] Please run previous notebooks and before you continue.\")\n",
    "    print(\"+\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "\n",
    "logger = _get_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, sagemaker\n",
    "sys.path.insert(0, \"./src\")\n",
    "#!{sys.executable} -m pip install -qU \"sagemaker>=2.45\"\n",
    "print(sagemaker.__version__)\n",
    "model_trace_name = 'model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1. Inference script\n",
    "---\n",
    "\n",
    "The code cell below stores the SageMaker inference script in the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/infer_pytorch_neo.py\n",
    "\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image  # Training container doesn't have this package\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    import neopytorch\n",
    "\n",
    "    logger.info(\"model_fn\")\n",
    "    neopytorch.config(model_dir=model_dir, neo_runtime=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # The compiled model is saved as \"compiled.pt\"\n",
    "    model = torch.jit.load(os.path.join(model_dir, \"compiled.pt\"), map_location=device)\n",
    "\n",
    "    # It is recommended to run warm-up inference during model load\n",
    "    sample_input_path = os.path.join(model_dir, \"sample_input.pkl\")\n",
    "    with open(sample_input_path, \"rb\") as input_file:\n",
    "        model_input = pickle.load(input_file)\n",
    "    if torch.is_tensor(model_input):\n",
    "        model_input = model_input.to(device)\n",
    "        model(model_input)\n",
    "    elif isinstance(model_input, tuple):\n",
    "        model_input = (inp.to(device) for inp in model_input if torch.is_tensor(inp))\n",
    "        model(*model_input)\n",
    "    else:\n",
    "        print(\"Only supports a torch tensor or a tuple of torch tensors\")\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def transform_fn(model, payload, request_content_type='application/octet-stream', \n",
    "                 response_content_type='application/json'):\n",
    "\n",
    "    logger.info('Invoking user-defined transform function')\n",
    "\n",
    "    if request_content_type != 'application/octet-stream':\n",
    "        raise RuntimeError(\n",
    "            'Content type must be application/octet-stream. Provided: {0}'.format(request_content_type))\n",
    "\n",
    "    # preprocess\n",
    "    decoded = Image.open(io.BytesIO(payload))\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[\n",
    "                0.485, 0.456, 0.406], std=[\n",
    "                0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    normalized = preprocess(decoded)\n",
    "    batchified = normalized.unsqueeze(0)\n",
    "\n",
    "    # predict\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batchified = batchified.to(device)\n",
    "    result = model.forward(batchified)\n",
    "\n",
    "    # Softmax (assumes batch size 1)\n",
    "    result = np.squeeze(result.detach().cpu().numpy())\n",
    "    result_exp = np.exp(result - np.max(result))\n",
    "    result = result_exp / np.sum(result_exp)\n",
    "\n",
    "    response_body = json.dumps(result.tolist())\n",
    "\n",
    "    return response_body, response_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. Load trained model\n",
    "---\n",
    "\n",
    "Load the trained model. In order to reduce compatibility issues with different framework versions and issues during serialization, it is recommended to initialize the model structure first and load the model weights rather than loading the entire model as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import tarfile\n",
    "import src.train_utils as train_utils\n",
    "\n",
    "classes, classes_dict = train_utils.get_classes(os.path.join(dataset_dir, 'valid'))\n",
    "num_classes = len(classes)\n",
    "model = train_utils.initialize_ft_model(base_model_name, num_classes=num_classes, feature_extract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load(os.path.join(local_model_path, model_name))\n",
    "model.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import tarfile\n",
    "\n",
    "input_shape = [1,3,224,224]\n",
    "dummy_input = torch.zeros(input_shape).float()\n",
    "dummy_input = dummy_input.to(device)\n",
    "trace = torch.jit.trace(model.float().eval(), dummy_input)\n",
    "trace.save(model_trace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference without Endpoint\n",
    "\n",
    "Debugging while performing inference in a local environment is recommended because there are many risks to directly deploying a trained model to a production environment without sufficient testing. Please refer to the code in the code cell below as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(img_path, predictor, classes_dict, show_img=True):\n",
    "    with open(img_path, mode='rb') as file:\n",
    "        payload = bytearray(file.read())\n",
    "\n",
    "    response = predictor.predict(payload)\n",
    "    result = json.loads(response.decode())\n",
    "    pred_cls_idx, pred_cls_str, prob = parse_result(result, classes_dict, show_img)\n",
    "    \n",
    "    return pred_cls_idx, pred_cls_str, prob \n",
    "\n",
    "def parse_result(result, classes_dict, show_img=True):\n",
    "    pred_cls_idx = np.argmax(result)\n",
    "    pred_cls_str = classes_dict[pred_cls_idx]\n",
    "    prob = np.amax(result)*100\n",
    "    \n",
    "    if show_img:\n",
    "        import cv2\n",
    "        import matplotlib.pyplot as plt\n",
    "        im = cv2.imread(img_path, cv2.COLOR_BGR2RGB)\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "        cv2.putText(im, f'{pred_cls_str} {prob:.2f}%', (10,40), font, 0.7, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(im[:,:,::-1])    \n",
    "\n",
    "    return pred_cls_idx, pred_cls_str, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model deployment is complete, let's do some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from src.infer_pytorch_neo import transform_fn\n",
    "\n",
    "model = torch.jit.load(model_trace_name)\n",
    "model = model.to(device)\n",
    "\n",
    "path = f\"./{dataset_dir}/valid/{classes[0]}\"\n",
    "img_list = os.listdir(path)\n",
    "img_path_list = [os.path.join(path, img) for img in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_idx = random.randint(0, len(img_list))\n",
    "img_path = img_path_list[test_idx]\n",
    "\n",
    "with open(img_path, mode='rb') as file:\n",
    "    payload = bytearray(file.read())\n",
    "    \n",
    "response_body, _ = transform_fn(model, payload)\n",
    "result = json.loads(response_body)\n",
    "parse_result(result, classes_dict, show_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Compile Model with SageMaker Neo\n",
    "---\n",
    "\n",
    "## Model Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('model.tar.gz', 'w:gz') as f:\n",
    "    f.add(model_trace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_for_jetson(role, bucket, dataset_dir, \n",
    "                             target_device='jetson-nano', trt_ver='7.1.3', cuda_ver='10.2', gpu_code='sm_53',\n",
    "                             framework='PYTORCH', base_model_name='model', img_size=224, use_gpu=True):\n",
    "    import time\n",
    "    import sagemaker\n",
    "    from sagemaker.utils import name_from_base\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    sess = sagemaker.Session()\n",
    "    region = sess.boto_region_name\n",
    "    target_device_ = target_device.replace('_', '-')\n",
    "\n",
    "    if use_gpu:\n",
    "        compilation_job_name = name_from_base(f'{target_device_}-{base_model_name}-gpu-pytorch')\n",
    "    else:\n",
    "        compilation_job_name = name_from_base(f'{target_device_}-{base_model_name}-cpu-pytorch')        \n",
    "    \n",
    "    s3_compiled_model_path = 's3://{}/{}/{}/neo-output'.format(bucket, dataset_dir, compilation_job_name)\n",
    "    key_prefix = f'{dataset_dir}/{compilation_job_name}/model'\n",
    "    s3_model_path = sess.upload_data(path='model.tar.gz', key_prefix=key_prefix)\n",
    "\n",
    "    # Configuration\n",
    "    if use_gpu:\n",
    "        input_config = {\n",
    "            'S3Uri': s3_model_path,\n",
    "            'DataInputConfig': f'{{\"input0\": [1,3,{img_size},{img_size}]}}',\n",
    "            'Framework': framework,\n",
    "        }\n",
    "        output_config = {\n",
    "            'S3OutputLocation': s3_compiled_model_path,\n",
    "            'TargetPlatform': { \n",
    "                'Os': 'LINUX', \n",
    "                'Arch': 'ARM64', # change this to X86_64 if you need\n",
    "                'Accelerator': 'NVIDIA'  # comment this if you don't have an Nvidia GPU\n",
    "            },        \n",
    "            # Jetson Xavier: sm_72; Jetson Nano: sm_53\n",
    "            'CompilerOptions': f'{{\"trt-ver\": \"{trt_ver}\", \"cuda-ver\": \"{cuda_ver}\", \"gpu-code\": \"{gpu_code}\"}}' # Jetpack 4.5.1            \n",
    "        }\n",
    "    else:\n",
    "        input_config = {\n",
    "            'S3Uri': s3_model_path,\n",
    "            'DataInputConfig': f'{{\"input0\": [1,3,{img_size},{img_size}]}}',\n",
    "            'Framework': framework,\n",
    "        }\n",
    "        output_config = {\n",
    "            'S3OutputLocation': s3_compiled_model_path,\n",
    "            'TargetPlatform': { \n",
    "                'Os': 'LINUX', \n",
    "                'Arch': 'ARM64', # change this to X86_64 if you need\n",
    "            },        \n",
    "        }        \n",
    "        \n",
    "    # Create Compilation job    \n",
    "    compilation_response = sm_client.create_compilation_job(\n",
    "        CompilationJobName=compilation_job_name,\n",
    "        RoleArn=role,\n",
    "        InputConfig=input_config,\n",
    "        OutputConfig=output_config,\n",
    "        StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    "    )\n",
    "    \n",
    "    return compilation_response, compilation_job_name, s3_compiled_model_path, s3_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, boto3, sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "compilation_cpu_response, compilation_cpu_job_name, s3_cpu_compiled_model_path, s3_cpu_model_path = compile_model_for_jetson(role, bucket, dataset_dir, use_gpu=False)\n",
    "compilation_gpu_response, compilation_gpu_job_name, s3_gpu_compiled_model_path, s3_gpu_model_path = compile_model_for_jetson(role, bucket, dataset_dir, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(compilation_cpu_response)\n",
    "logger.info(compilation_gpu_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker')\n",
    "compilation_jobs = [compilation_gpu_job_name, compilation_gpu_job_name]\n",
    "\n",
    "max_time = time.time() + 15*60 # 15 mins\n",
    "for job in compilation_jobs:\n",
    "    while time.time() < max_time:\n",
    "        resp = sm_client.describe_compilation_job(CompilationJobName=job)    \n",
    "        if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "            print('Running...')\n",
    "        else:\n",
    "            print(resp['CompilationJobStatus'], job)\n",
    "            break\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "compilation_job_name = name_from_base(f'{target_device_}-{base_model_name}-cpu-pytorch')\n",
    "s3_compiled_model_path = 's3://{}/{}/{}/neo-output'.format(bucket, dataset_dir, compilation_job_name)\n",
    "key_prefix = f'{dataset_dir}/{compilation_job_name}/model'\n",
    "s3_model_path = sess.upload_data(path='model.tar.gz', key_prefix=key_prefix)\n",
    "\n",
    "compilation_response = sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_model_path,\n",
    "        'DataInputConfig': f'{{\"input0\": [1,3,{img_size},{img_size}]}}',\n",
    "        'Framework': framework,\n",
    "        #'FrameworkVersion': framework_version\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': s3_compiled_model_path,\n",
    "        'TargetPlatform': { \n",
    "            'Os': 'LINUX', \n",
    "            'Arch': 'ARM64', # change this to X86_64 if you need\n",
    "        },\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_ecr_accid = {'us-east-1':'785573368785',\n",
    "    'us-east-2':'007439368137',\n",
    "    'us-west-1':'710691900526',\n",
    "    'us-west-2':'301217895009',\n",
    "    'ap-northeast-1':'941853720454',\n",
    "    'ap-northeast-2':'151534178276'\n",
    "}\n",
    "\n",
    "ecr_cpu_uri = f'{neo_ecr_accid[region]}.dkr.ecr.{region}.amazonaws.com/sagemaker-inference-pytorch:{framework_version}-cpu-py3'\n",
    "ecr_gpu_uri = f'{neo_ecr_accid[region]}.dkr.ecr.{region}.amazonaws.com/sagemaker-inference-pytorch:{framework_version}-gpu-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && tar -cvzf ../sourcecode.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_src_path = 's3://{}/{}/{}/src/sourcecode.tar.gz'.format(bucket, dataset_dir, compilation_job_name)\n",
    "print(s3_src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp sourcecode.tar.gz {s3_src_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path, s3_compiled_model_path, s3_src_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_model_name = f'model-{compilation_job_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_path2 = os.path.join(s3_compiled_model_path, 'model-ml_c5.tar.gz')\n",
    "print(compiled_model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key\n",
    "# Value\n",
    "# MMS_DEFAULT_RESPONSE_TIMEOUT\t500\n",
    "# SAGEMAKER_CONTAINER_LOG_LEVEL\t20a\n",
    "# SAGEMAKER_PROGRAM\tinfer_pytorch_neo.py\n",
    "# SAGEMAKER_REGION\tus-east-1\n",
    "# SAGEMAKER_SUBMIT_DIRECTORY\ts3://sagemaker-us-east-1-143656149352/sagemaker-inference-pytorch-2021-08-01-09-33-14-680/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sagemaker model\n",
    "create_model_api_response = sm_client.create_model(\n",
    "                                    ModelName=deployment_model_name,\n",
    "                                    PrimaryContainer={\n",
    "                                        'Image': ecr_cpu_uri,\n",
    "                                        'ModelDataUrl': compiled_model_path2,\n",
    "\n",
    "                                        \"Environment\": {\n",
    "                                            \"SAGEMAKER_PROGRAM\": \"infer_pytorch_neo.py\",\n",
    "                                            #\"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/src\",\n",
    "                                            \"SAGEMAKER_SUBMIT_DIRECTORY\": s3_src_path,\n",
    "                                            \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "                                            \"SAGEMAKER_REGION\": region,\n",
    "                                            \"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"\n",
    "                                        }                                            \n",
    "                                    },\n",
    "                                    ExecutionRoleArn=role\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compilation_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f'endpoint-config-ml-c5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sagemaker endpoint config\n",
    "instance_type = 'ml.c5.xlarge'\n",
    "variant_name = 'AllTraffic'\n",
    "create_endpoint_config_api_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': instance_type,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': deployment_model_name,\n",
    "        'VariantName': variant_name\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"create_endpoint_config API response\", create_endpoint_config_api_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'endpoint-ml-c5'\n",
    "#logger.info(f\"Creating endpoint\")        \n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "\n",
    "print (\"create_endpoint API response\", create_endpoint_response)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_info = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = endpoint_info['EndpointStatus']\n",
    "endpoint_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################\n",
    "# ## endpoint 생성\n",
    "# #########################################\n",
    "\n",
    "# if not existing_endpoints:\n",
    "#     logger.info(f\"Creating endpoint\")        \n",
    "#     create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "#         EndpointName=args.endpoint_name,\n",
    "#         EndpointConfigName=endpoint_config_name)\n",
    "# else:\n",
    "#     logger.info(f\"Endpoint exists\")            \n",
    "    \n",
    "\n",
    "# endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "# endpoint_status = endpoint_info['EndpointStatus']\n",
    "\n",
    "\n",
    "# logger.info(f'Endpoint status is creating')    \n",
    "# while endpoint_status == 'Creating':\n",
    "#     endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "#     endpoint_status = endpoint_info['EndpointStatus']\n",
    "#     logger.info(f'Endpoint status: {endpoint_status}')\n",
    "#     if endpoint_status == 'Creating':\n",
    "#         time.sleep(30)\n",
    "\n",
    "while endpoint_status == 'Creating':\n",
    "    endpoint_info = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = endpoint_info['EndpointStatus']\n",
    "    print(f'Endpoint status: {endpoint_status}')\n",
    "    if endpoint_status == 'Creating':\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "existing_endpoints = sm_client.list_endpoints(NameContains=endpoint_name)['Endpoints']\n",
    "print(existing_endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializer = sagemaker.serializers.IdentitySerializer(content_type='application/octet-stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import Predictor\n",
    "\n",
    "# sagemaker.serializers.IdentitySerializer(content_type='application/octet-stream')\n",
    "# #endpoint = 'insert name of your endpoint here'\n",
    "\n",
    "# # # Read image into memory\n",
    "# # payload = None\n",
    "# # with open(\"image.jpg\", 'rb') as f:\n",
    "# #     payload = f.read()\n",
    "\n",
    "# # Type:           Predictor\n",
    "# # String form:    <sagemaker.predictor.Predictor object at 0x7ff780b7de48>\n",
    "# # File:           ~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\n",
    "# # Docstring:      Make prediction requests to an Amazon SageMaker endpoint.\n",
    "# # Init docstring:\n",
    "# # Initialize a ``Predictor``.\n",
    "\n",
    "# # Behavior for serialization of input data and deserialization of\n",
    "# # result data can be configured through initializer arguments. If not\n",
    "# # specified, a sequence of bytes is expected and the API sends it in the\n",
    "# # request body without modifications. In response, the API returns the\n",
    "# # sequence of bytes from the prediction result without any modifications.\n",
    "\n",
    "# # Args:\n",
    "# #     endpoint_name (str): Name of the Amazon SageMaker endpoint to which\n",
    "# #         requests are sent.\n",
    "# #     sagemaker_session (sagemaker.session.Session): A SageMaker Session\n",
    "# #         object, used for SageMaker interactions (default: None). If not\n",
    "# #         specified, one is created using the default AWS configuration\n",
    "# #         chain.\n",
    "# #     serializer (:class:`~sagemaker.serializers.BaseSerializer`): A\n",
    "# #         serializer object, used to encode data for an inference endpoint\n",
    "# #         (default: :class:`~sagemaker.serializers.IdentitySerializer`).\n",
    "# #     deserializer (:class:`~sagemaker.deserializers.BaseDeserializer`): A\n",
    "# #         deserializer object, used to decode data from an inference\n",
    "# #         endpoint (default: :class:`~sagemaker.deserializers.BytesDeserializer`)\n",
    "    \n",
    "# predictor = Predictor(endpoint_name, serializer=serializer)\n",
    "# inference_response = predictor.predict(data=payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_runtime_client = boto3.client('sagemaker-runtime')\n",
    "response = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/octet-stream',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "# endpoint_name = model.endpoint_name\n",
    "\n",
    "# response = runtime_client.invoke_endpoint(\n",
    "#     EndpointName=endpoint_name, \n",
    "#     ContentType='application/x-image',\n",
    "#     Accept='application/json',\n",
    "#     Body=img_byte\n",
    "#     )\n",
    "# outputs = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import boto3\n",
    "# import argparse\n",
    "# import sys, os\n",
    "\n",
    "# import logging\n",
    "# import logging.handlers\n",
    "\n",
    "# def _get_logger():\n",
    "#     '''\n",
    "#     로깅을 위해 파이썬 로거를 사용\n",
    "#     # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "#     '''\n",
    "#     loglevel = logging.DEBUG\n",
    "#     l = logging.getLogger(__name__)\n",
    "#     if not l.hasHandlers():\n",
    "#         l.setLevel(loglevel)\n",
    "#         logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "#         l.handler_set = True\n",
    "#     return l  \n",
    "\n",
    "# logger = _get_logger()\n",
    "\n",
    "\n",
    "# #########################################\n",
    "# ## 커맨드 인자 처리\n",
    "# #########################################\n",
    "\n",
    "# # Parse argument variables passed via the DeployModel processing step\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model_name', type=str)\n",
    "# parser.add_argument('--region', type=str, default=\"ap-northeast-2\")\n",
    "# parser.add_argument('--endpoint_instance_type', type=str, default='ml.t3.medium')\n",
    "# parser.add_argument('--endpoint_name', type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# logger.info(\"#############################################\")\n",
    "# logger.info(f\"args.model_name: {args.model_name}\")\n",
    "# logger.info(f\"args.region: {args.region}\")    \n",
    "# logger.info(f\"args.endpoint_instance_type: {args.endpoint_instance_type}\")        \n",
    "# logger.info(f\"args.endpoint_name: {args.endpoint_name}\")    \n",
    "\n",
    "# region = args.region\n",
    "# instance_type = args.endpoint_instance_type\n",
    "# model_name = args.model_name\n",
    "\n",
    "\n",
    "# boto3.setup_default_session(region_name=region)\n",
    "# sagemaker_boto_client = boto3.client('sagemaker')\n",
    "\n",
    "# #name truncated per sagameker length requirememnts (63 char max)\n",
    "# endpoint_config_name=f'{args.model_name[:56]}-config'\n",
    "# existing_configs = sagemaker_boto_client.list_endpoint_configs(NameContains=endpoint_config_name)['EndpointConfigs']\n",
    "\n",
    "# #########################################\n",
    "# ## endpoint_config 생성\n",
    "# #########################################\n",
    "\n",
    "# if not existing_configs:\n",
    "#     create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "#         EndpointConfigName=endpoint_config_name,\n",
    "#         ProductionVariants=[{\n",
    "#             'InstanceType': instance_type,\n",
    "#             'InitialVariantWeight': 1,\n",
    "#             'InitialInstanceCount': 1,\n",
    "#             'ModelName': model_name,\n",
    "#             'VariantName': 'AllTraffic'\n",
    "#         }]\n",
    "#     )\n",
    "\n",
    "# existing_endpoints = sagemaker_boto_client.list_endpoints(NameContains=args.endpoint_name)['Endpoints']\n",
    "\n",
    "# #########################################\n",
    "# ## endpoint 생성\n",
    "# #########################################\n",
    "\n",
    "# if not existing_endpoints:\n",
    "#     logger.info(f\"Creating endpoint\")        \n",
    "#     create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "#         EndpointName=args.endpoint_name,\n",
    "#         EndpointConfigName=endpoint_config_name)\n",
    "# else:\n",
    "#     logger.info(f\"Endpoint exists\")            \n",
    "    \n",
    "\n",
    "# endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "# endpoint_status = endpoint_info['EndpointStatus']\n",
    "\n",
    "\n",
    "# logger.info(f'Endpoint status is creating')    \n",
    "# while endpoint_status == 'Creating':\n",
    "#     endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "#     endpoint_status = endpoint_info['EndpointStatus']\n",
    "#     logger.info(f'Endpoint status: {endpoint_status}')\n",
    "#     if endpoint_status == 'Creating':\n",
    "#         time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create the endpoint configuration 'testEndPointConfig'\n",
    "# response = client.create_endpoint_config(\n",
    "#     EndpointConfigName='testEndPointConfig',\n",
    "#     ProductionVariants=[\n",
    "#         {\n",
    "#             'VariantName': 'model1',\n",
    "#             'ModelName': 'sagemaker-mxnet-2018-10-08-20-54-00-482',\n",
    "#             'InitialInstanceCount': 1,\n",
    "#             'InstanceType': 'ml.t2.medium',\n",
    "#             'InitialVariantWeight': 1\n",
    "#         },\n",
    "#         {\n",
    "#             'VariantName': 'model2',\n",
    "#             'ModelName': 'sagemaker-mxnet-2018-10-10-20-54-00-482',\n",
    "#             'InitialInstanceCount': 1,\n",
    "#             'InstanceType': 'ml.t2.medium',\n",
    "#             'InitialVariantWeight': 1\n",
    "#         },\n",
    "#     ],\n",
    "#     Tags=[\n",
    "#         {\n",
    "#             'Key': 'test_key_ab1',\n",
    "#             'Value': 'test_value_ab1'\n",
    "#         },\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "#     if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "#         print('Compiling Running...')\n",
    "#     else:\n",
    "#         print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "#         break\n",
    "#     time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role = \"arn:aws:iam::143656149352:role/service-role/AmazonSageMaker-ExecutionRole-20210315T163355\"\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "compilation_job_name = name_from_base(f'Neo-{base_model_name}')\n",
    "prefix = compilation_job_name + '/model'\n",
    "\n",
    "s3_model_path = sess.upload_data(path='model.tar.gz', key_prefix=prefix)\n",
    "\n",
    "data_shape = '{\"input0\":[1,3,224,224]}'\n",
    "#target_device = 'ml_c5'\n",
    "target_device = 'jetson_nano'\n",
    "\n",
    "framework = 'PYTORCH'\n",
    "framework_version = '1.6'\n",
    "compiled_model_path = 's3://{}/{}/output'.format(bucket, compilation_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "sagemaker_model = PyTorchModel(model_data=s3_model_path,\n",
    "                               predictor_cls=Predictor,\n",
    "                               framework_version=framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               source_dir='src',\n",
    "                               entry_point='infer_pytorch_neo.py',\n",
    "                               py_version='py3',\n",
    "                               env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'}\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "Compile the model with SageMaker Neo. The compiled model is stored in s3 and can be deployed directly to the target edge device or target instance. When deploying to a target edge device, the compiled model can be inferred using the Neo-DLR package. The path to the compiled model artifacts can also be found in the SageMaker UI on the AWS management console and can also be imported as `compiled_model.model_data`.\n",
    "\n",
    "Reference: https://github.com/neo-ai/neo-ai-dlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_model.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compiled_model = sagemaker_model.compile(target_instance_family=target_device, \n",
    "                                         input_shape=data_shape,\n",
    "                                         job_name=compilation_job_name,\n",
    "                                         role=role,\n",
    "                                         framework=framework.lower(),\n",
    "                                         framework_version=framework_version,\n",
    "                                         output_path=compiled_model_path,\n",
    "                                         compiler_options=\"{'gpu-code': 'sm_53', 'trt-ver': '7.1.3', 'cuda-ver': '10.2'}\",\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_neo_model_path = compiled_model.model_data\n",
    "%store s3_neo_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "If you have already compiled with SageMaker Neo, you can initialize PyTorch Estimator right away (See code snippets below). Just change the `model_data` argument to the S3 path containing the model artifact compiled with Neo.\n",
    "\n",
    "```python\n",
    "compiled_model = PyTorchModel(model_data=compiled_model_path,\n",
    "                               predictor_cls=Predictor,\n",
    "                               framework_version = framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               source_dir='src',\n",
    "                               entry_point='infer_pytorch_neo.py',\n",
    "                               py_version='py3',\n",
    "                               env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'}\n",
    "                              )\n",
    "```                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. (Optional) Deploy Model\n",
    "\n",
    "---\n",
    "\n",
    "Deploy the compiled model. It takes about 5-10 minutes to deploy the real-time endpoint, as it provisions the deployment cluster managed by SageMaker and initialize the SageMaker inference container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "The `deploy(.)` provided by the SageMaker Python API creates `Model, EndpointConfig, and Endpoint` internally. We recommend using the boto3 API for production purposes rather than testing and education purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "compiled_model = PyTorchModel(model_data=f'{compiled_model_path}/model-LINUX_ARM64_NVIDIA.tar.gz',\n",
    "                               predictor_cls=Predictor,\n",
    "                               framework_version=framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               source_dir='src',\n",
    "                               entry_point='infer_pytorch_neo.py',\n",
    "                               py_version='py3',\n",
    "                               env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'}\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = compiled_model.deploy(initial_instance_count = 1,\n",
    "                                  instance_type = 'ml.c5.xlarge', wait=False\n",
    "                                 )\n",
    "endpoint_name = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker Endpoint</a> After About 5 Minutes</b>'.format(\n",
    "            region, endpoint_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Once the endpoint is successfully created, let's do some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "path = f\"./{dataset_dir}/valid/{classes[0]}\"\n",
    "img_list = os.listdir(path)\n",
    "img_path_list = [os.path.join(path, img) for img in img_list]\n",
    "num_imgs = len(img_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change test_idx and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = random.randint(0, num_imgs-1)\n",
    "img_path = img_path_list[test_idx]\n",
    "pred_cls_idx, pred_cls_str, prob = get_inference(img_path, predictor, classes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures latency for multiple input data. In many cases, performing inference on CPU instances alone is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    response = predictor.predict(payload)\n",
    "neo_inference_time = (time.time()-start)\n",
    "print(f'Neo optimized inference time is {neo_inference_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Clean-up\n",
    "\n",
    "To prevent billing due to the SageMaker Endpoint, be sure to delete the Endpoint after this Hands-on is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -czvf neo_model.tar.gz neo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dlr\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from dlr import DLRModel\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     cvimage = cv2.resize(image, (224,224))\n",
    "#     #config_utils.logger.info(\"img shape after resize: '{}'.\".format(cvimage.shape))\n",
    "\n",
    "#     img = np.asarray(cvimage, dtype='float32')\n",
    "#     img /= 255.0 # scale 0 to 1\n",
    "#     mean = np.array([0.485, 0.456, 0.406]) \n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     img = (img - mean) / std\n",
    "#     img = np.transpose(img, (2,0,1)) \n",
    "#     img = np.expand_dims(img, axis=0) # e.g., [1x3x224x224]\n",
    "#     return img\n",
    "\n",
    "#     #config_utils.logger.info(\"img shape final: '{}'.\".format(img.shape))\n",
    "\n",
    "#     #predict(img)\n",
    "    \n",
    "# image_data = cv2.imread('raw/red_normal/red_raw_normal_0001.jpeg')\n",
    "# image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "# image_data = preprocess_image(image_data)\n",
    "\n",
    "# device = 'cpu'            \n",
    "# model = DLRModel('neo_model', device)\n",
    "# model.run(image_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_data = cv2.imread('raw/red_normal/red_raw_normal_0001.jpeg')\n",
    "# image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "# image_data = preprocess_image(image_data)\n",
    "\n",
    "# device = 'cpu'            \n",
    "# model = DLRModel('neo_model', device)\n",
    "# model.run(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
